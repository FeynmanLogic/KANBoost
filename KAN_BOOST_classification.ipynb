{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHPqAfoY0TO0"
      },
      "source": [
        "# Implementation of a KANBoost\n",
        "## Initialisations\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1n3Plr3bvLI",
        "outputId": "d7391521-0312-4849-8a2f-7bd20ca44bcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pykan in /usr/local/lib/python3.10/dist-packages (0.2.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install pykan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qrVEGBzEcUQc",
        "outputId": "5870f5d8-4296-4aa3-ddd9-7cfe404d6c5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from kan import *\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "else:\n",
        "  device = torch.device(\"cpu\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RE1svm9cXkX"
      },
      "source": [
        "## Loading the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DeltaPrefetcherModel:\n",
        "    def __init__(self, page_size, block_size):\n",
        "        self.page_size = page_size\n",
        "        self.block_size = block_size\n",
        "\n",
        "    def ensure_48bit_address(self, load_address):\n",
        "        # Ensure the load_address is a 48-bit binary string\n",
        "        return bin(int(load_address, 16))[2:].zfill(48)\n",
        "\n",
        "    def calculate_delta(self, block1, block2):\n",
        "        # Calculate the delta between two blocks (binary subtraction)\n",
        "        return int(block1, 2) - int(block2, 2)\n",
        "\n",
        "    def split_load_address(self, line):\n",
        "        instr_id, cycle_count, load_address, instr_ptr, llc_hit_miss = line\n",
        "        binary_address = self.ensure_48bit_address(load_address)\n",
        "\n",
        "        page = binary_address[:self.page_size]  # x Bit (Varies)\n",
        "        block = binary_address[self.page_size:self.page_size + self.block_size]  # 6 Bit Fixed\n",
        "        block_offset = binary_address[self.page_size + self.block_size:]  # Remaining bits\n",
        "\n",
        "        return (instr_id, page, block, block_offset)\n",
        "\n",
        "    def delta_to_one_hot(self, delta):\n",
        "        # Create a 128-dimensional array initialized with zeros\n",
        "        one_hot = [0] * 128\n",
        "        # Calculate the correct position (64 + delta)\n",
        "        position = 64 + delta\n",
        "        # Set the corresponding position to 1\n",
        "        one_hot[position] = 1\n",
        "        return one_hot\n",
        "\n",
        "    def preprocess_data(self, data):\n",
        "        input_features = []\n",
        "        output_labels = []\n",
        "        page_blocks = {}\n",
        "\n",
        "        for i in range(len(data) - 1):\n",
        "            instr_id, current_page, current_block, current_block_offset = self.split_load_address(data[i])\n",
        "            _, next_page, next_block, next_block_offset = self.split_load_address(data[i + 1])\n",
        "\n",
        "            # Initialize page_blocks if current_page is not present\n",
        "            if current_page not in page_blocks:\n",
        "                page_blocks[current_page] = ['000001']\n",
        "\n",
        "            # Calculate delta values for the past blocks\n",
        "            delta1 = delta2 = delta3 = 1\n",
        "\n",
        "            if len(page_blocks[current_page]) > 1:\n",
        "                delta1 = self.calculate_delta(page_blocks[current_page][-1], page_blocks[current_page][-2])\n",
        "            if len(page_blocks[current_page]) > 2:\n",
        "                delta2 = self.calculate_delta(page_blocks[current_page][-2], page_blocks[current_page][-3])\n",
        "            if len(page_blocks[current_page]) > 3:\n",
        "                delta3 = self.calculate_delta(page_blocks[current_page][-3], page_blocks[current_page][-4])\n",
        "\n",
        "            # Calculate delta for the next block (relative to current block)\n",
        "            next_delta = self.calculate_delta(next_block, current_block)\n",
        "\n",
        "            # Append input features\n",
        "            input_features.append((instr_id, int(current_block, 2), delta1, delta2, delta3))\n",
        "\n",
        "            # Convert next_delta to a 128-dimensional one-hot array and append as the output label\n",
        "            output_labels.append(next_delta+64)\n",
        "\n",
        "            # Append the current block to the page's block list\n",
        "            page_blocks[current_page].append(current_block)\n",
        "\n",
        "        return input_features, output_labels\n",
        "\n",
        "# Reading the data from text file (same as before)\n",
        "def read_data_from_file(filename):\n",
        "    data = []\n",
        "    with open(filename, 'r') as file:\n",
        "        for line in file:\n",
        "            line = line.strip()  # Remove any extra spaces or newline characters\n",
        "            if line:  # Skip empty lines\n",
        "                # Split by comma and remove any extra spaces\n",
        "                fields = [x.strip() for x in line.split(',')]\n",
        "                instr_id = int(fields[0])\n",
        "                cycle_count = int(fields[1])\n",
        "                load_address = fields[2]\n",
        "                instr_ptr = fields[3]\n",
        "                llc_hit_miss = int(fields[4])\n",
        "\n",
        "                # Append as a tuple\n",
        "                data.append((instr_id, cycle_count, load_address, instr_ptr, llc_hit_miss))\n",
        "    return data\n",
        "\n",
        "# Usage (same as before)\n",
        "filename = '/content/new_DS.txt'  # Replace with your actual file path\n",
        "data = read_data_from_file(filename)\n",
        "\n",
        "# Initialize the DeltaPrefetcherModel\n",
        "page_size = 36  # Replace with your actual page size (bits)\n",
        "block_size = 6  # Replace with your actual block size (bits)\n",
        "\n",
        "model = DeltaPrefetcherModel(page_size, block_size)\n",
        "\n",
        "# Use the preprocess_data function to process the entire data\n",
        "input_features, output_labels = model.preprocess_data(data)\n"
      ],
      "metadata": {
        "id": "TUAWsM-U8ilW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMwDaT0icUJD"
      },
      "outputs": [],
      "source": [
        "def load_dataset():\n",
        "\n",
        "    data = input_features\n",
        "    target = output_labels\n",
        "\n",
        "    # Convert to PyTorch tensors\n",
        "    data_tensor = torch.tensor(data, dtype=torch.float32)\n",
        "    target_tensor = torch.tensor(target, dtype=torch.long) #This needs to be torch.float32\n",
        "\n",
        "    # Split dataset into train and test sets\n",
        "    train_data, test_data, train_target, test_target = train_test_split(data_tensor, target_tensor, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Create data loaders (optional, if you want to batch and shuffle the data)\n",
        "    train_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(train_data, train_target), batch_size=1, shuffle=True)\n",
        "    test_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(test_data, test_target), batch_size=1, shuffle=False)\n",
        "\n",
        "    train_inputs = torch.empty(0, 5, device=device)\n",
        "    train_labels = torch.empty(0, dtype=torch.long,device=device)\n",
        "    test_inputs = torch.empty(0, 5, device=device)\n",
        "    test_labels = torch.empty(0,dtype=torch.long,  device=device)\n",
        "\n",
        "    # Concatenate all data into a single tensor on the specified device\n",
        "    for data, labels in train_loader:\n",
        "        train_inputs = torch.cat((train_inputs, data.to(device)), dim=0)\n",
        "        train_labels = torch.cat((train_labels, labels.to(device)), dim=0)\n",
        "\n",
        "    for data, labels in test_loader:\n",
        "        test_inputs = torch.cat((test_inputs, data.to(device)), dim=0)\n",
        "        test_labels = torch.cat((test_labels, labels.to(device)), dim=0)\n",
        "\n",
        "    dataset = {}\n",
        "    dataset['train_input'] = train_inputs\n",
        "    dataset['test_input'] = test_inputs\n",
        "    dataset['train_label'] = train_labels\n",
        "    dataset['test_label'] = test_labels\n",
        "\n",
        "    return dataset\n",
        "\n",
        "traces_dataset = load_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wDBtdgGocUHM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64464afe-18fb-4301-c003-a9fd37cb7994"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data shape: torch.Size([16605, 5])\n",
            "Train target shape: torch.Size([16605])\n",
            "Test data shape: torch.Size([4152, 5])\n",
            "Test target shape: torch.Size([4152])\n",
            "====================================\n"
          ]
        }
      ],
      "source": [
        "print(\"Train data shape: {}\".format(traces_dataset['train_input'].shape))\n",
        "print(\"Train target shape: {}\".format(traces_dataset['train_label'].shape))\n",
        "print(\"Test data shape: {}\".format(traces_dataset['test_input'].shape))\n",
        "print(\"Test target shape: {}\".format(traces_dataset['test_label'].shape))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7ERosp1iM17"
      },
      "source": [
        "## Creating and Training the KAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkjQDBTnNFHw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d311cbb6-1cb3-4551-bf75-6292223ef824"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "checkpoint directory created: ./model\n",
            "saving model version 0.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 14320.0703,  43107.1406,  10604.2236,  ...,  47133.6484,\n",
              "          26611.8516, -11398.8232],\n",
              "        [ 21378.7598,  64362.2109,  15829.4883,  ...,  70367.2188,\n",
              "          39730.7109, -17019.0059],\n",
              "        [ 24095.5332,  72544.8438,  17840.0410,  ...,  79309.6875,\n",
              "          44780.3125, -19181.7090],\n",
              "        ...,\n",
              "        [  7284.3921,  21917.9727,   5397.5762,  ...,  23977.3926,\n",
              "          13536.5020,  -5797.0791],\n",
              "        [ 47148.5469, 141947.6250,  34909.1484,  ..., 155187.0938,\n",
              "          87622.2188, -37534.2734],\n",
              "        [ 13047.6621,  39279.3945,   9661.4541,  ...,  42946.3633,\n",
              "          24248.1523, -10386.2217]], device='cuda:0', grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "model = KAN(width=[5, 64,128], grid=3, k=3, seed=0, device=device)\n",
        "model(traces_dataset['train_input'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhNKU6T1iLWe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cee6f49a-31b1-4da2-99e8-fa5cb27aca27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "| train_loss: 2.16e+01 | test_loss: 2.20e+01 | reg: 3.85e+02 | : 100%|â–ˆ| 5000/5000 [50:39<00:00,  1."
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saving model version 0.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "def train_acc():\n",
        "    return torch.mean((torch.argmax(model(traces_dataset['train_input']), dim=1) == traces_dataset['train_label']).float())\n",
        "\n",
        "def test_acc():\n",
        "    return torch.mean((torch.argmax(model(traces_dataset['test_input']), dim=1) == traces_dataset['test_label']).float())\n",
        "\n",
        "results = model.fit(traces_dataset, opt=\"Adam\", metrics=(train_acc, test_acc),\n",
        "                      loss_fn=torch.nn.CrossEntropyLoss(), steps=5000, lamb=0.01, lamb_entropy=10.05, save_fig=False, img_folder=image_folder)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8UbnTTY00hQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1e1d699-1702-4da4-da25-bbe3761a1208"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.15850646793842316, 0.15751445293426514)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "results['train_acc'][-1], results['test_acc'][-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9k_bxhZnyM3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9bb9ebf-0946-4d9c-fcf3-9807cac44341"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saving model version 0.2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
              "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0',\n",
              "       grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "model = model.prune()\n",
        "model(traces_dataset['train_input'])\n",
        "# model.plot(scale=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0Z6nsb0n3Dm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "37e7608e-aadb-4a30-8ed1-e0e80d8b1d82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "description:   0%|                                                           | 0/50 [00:00<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 2.95 GiB. GPU 0 has a total capacity of 14.75 GiB of which 1.62 GiB is free. Process 105292 has 13.13 GiB memory in use. Of the allocated memory 12.03 GiB is allocated by PyTorch, and 954.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-21da7b73b86a>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# fine tune\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m results_1 = model.fit(traces_dataset, opt=\"Adam\", metrics=(train_acc, test_acc),\n\u001b[0m\u001b[1;32m      3\u001b[0m                       loss_fn=torch.nn.CrossEntropyLoss(), steps=50, lamb=0.01, lamb_entropy=10.)\n\u001b[1;32m      4\u001b[0m \u001b[0mresults_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/kan/MultKAN.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, opt, steps, log, lamb, lamb_l1, lamb_entropy, lamb_coef, lamb_coefdiff, update_grid, grid_update_num, loss_fn, lr, start_grid_update_step, stop_grid_update_step, batch, metrics, save_fig, in_vars, out_vars, beta, save_fig_freq, img_folder, singularity_avoiding, y_th, reg_metric, display_metrics)\u001b[0m\n\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mgrid_update_freq\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mstop_grid_update_step\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mupdate_grid\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mstart_grid_update_step\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1530\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_input'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1532\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"LBFGS\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/kan/MultKAN.py\u001b[0m in \u001b[0;36mupdate_grid\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    697\u001b[0m         \u001b[0mcall\u001b[0m \u001b[0mupdate_grid_from_samples\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mThis\u001b[0m \u001b[0mseems\u001b[0m \u001b[0munnecessary\u001b[0m \u001b[0mbut\u001b[0m \u001b[0mwe\u001b[0m \u001b[0mretain\u001b[0m \u001b[0mit\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msake\u001b[0m \u001b[0mof\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mmight\u001b[0m \u001b[0minherit\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mMultKAN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m         '''\n\u001b[0;32m--> 699\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_grid_from_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize_grid_from_another_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/kan/MultKAN.py\u001b[0m in \u001b[0;36mupdate_grid_from_samples\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdepth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_act\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_fun\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_grid_from_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/kan/KANLayer.py\u001b[0m in \u001b[0;36mupdate_grid_from_samples\u001b[0;34m(self, x, mode)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextend_grid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_extend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurve2coef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize_grid_from_parent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sample'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/kan/spline.py\u001b[0m in \u001b[0;36mcurve2coef\u001b[0;34m(x_eval, y_eval, grid, k, lamb)\u001b[0m\n\u001b[1;32m    113\u001b[0m                              \u001b[0;31m#driver='gelsy' if device == 'cpu' else 'gels').solution[:,:,:,0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m     \u001b[0mXtX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ijmn,ijnp->ijmp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m     \u001b[0mXty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ijmn,ijnp->ijmp'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0mn1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXtX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXtX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mXtX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/functional.py\u001b[0m in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;31m# the path for contracting 0 or 1 time(s) is already optimized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0;31m# or the user has disabled using opt_einsum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperands\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.95 GiB. GPU 0 has a total capacity of 14.75 GiB of which 1.62 GiB is free. Process 105292 has 13.13 GiB memory in use. Of the allocated memory 12.03 GiB is allocated by PyTorch, and 954.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ],
      "source": [
        "# fine tune\n",
        "results_1 = model.fit(traces_dataset, opt=\"Adam\", metrics=(train_acc, test_acc),\n",
        "                      loss_fn=torch.nn.CrossEntropyLoss(), steps=50, lamb=0.01, lamb_entropy=10.)\n",
        "results_1['train_acc'][-1], results_1['test_acc'][-1]"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m4qv3tV5a-yB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyrcyf8JuAuB"
      },
      "outputs": [],
      "source": [
        "lib = ['x','x^2','x^3','x^4','exp','log','sqrt','tanh','sin','tan','abs']\n",
        "model.auto_symbolic(lib=lib)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kkEQdHhVaIA6"
      },
      "outputs": [],
      "source": [
        "formula1, formula2, formula3 = model.symbolic_formula()[0] x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qLP-rzBbYNQ"
      },
      "outputs": [],
      "source": [
        "formula1"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "CBD58aME1Rvd",
        "tbhg0iWQ1FX-"
      ],
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}